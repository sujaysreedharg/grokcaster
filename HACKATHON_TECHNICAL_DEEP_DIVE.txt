================================================================================
                    GROKCASTER - TECHNICAL DEEP DIVE
                    For Hackathon Judges & Reviewers
================================================================================

OVERVIEW
--------
Grokcaster is a Chrome extension that transforms any webpage into a personalized 
AI-generated podcast using xAI's Grok ecosystem. The entire project runs natively 
on Grok - we use THREE distinct xAI APIs working together, plus optional X (Twitter) 
API integration for personalization. No third-party AI services are used.


================================================================================
                         GROK APIS USED
================================================================================

1. GROK CHAT API (Script Generation)
   Endpoint: POST https://api.x.ai/v1/chat/completions
   Model: grok-4-1-fast-non-reasoning
   Purpose: Generates the podcast script dialogue between two hosts

2. GROK TTS API (Text-to-Speech)
   Endpoint: POST https://api.x.ai/v1/audio/speech
   Voices: Ara (female), Rex (male)
   Purpose: Converts the generated script into spoken audio

3. GROK REALTIME VOICE API (Live Talk)
   Endpoint: WSS wss://api.x.ai/v1/realtime
   Protocol: WebSocket with PCM16 audio
   Purpose: Real-time voice conversation with Grok

4. X (TWITTER) API (Optional Personalization)
   Endpoint: https://api.twitter.com/2/users/me
   Purpose: Fetches user interests to personalize content


================================================================================
                    HOW GROK CHAT API IS USED
================================================================================

When the user clicks "Generate Podcast", here's exactly what happens:

STEP 1: CONTENT EXTRACTION
The content script extracts text from the current webpage. If the user has used 
the snipping tool, we use that selected content instead. We take up to 10,000 
characters of page text.

STEP 2: CONTEXT BUILDING
The context_builder.py module constructs a detailed prompt. This prompt includes:
- The extracted page content (truncated to ~3000 chars for the API)
- The page title for context
- User's X interests if personalization is enabled
- Duration requirements (how many words/lines to generate)
- Tone instructions (formal, casual, unhinged, etc.)
- Mode instructions (podcast, summary, debate)

STEP 3: API CALL TO GROK CHAT
We send a POST request to the Grok Chat API with:

{
  "model": "grok-4-1-fast-non-reasoning",
  "messages": [
    {"role": "system", "content": "<system prompt with personality>"},
    {"role": "user", "content": "<constructed prompt with page content>"}
  ],
  "temperature": 0.8  // or 1.2 for unhinged mode
}

The system prompt defines the two podcast hosts:
- Alex: Female, curious, asks questions
- Sam: Male, knowledgeable, provides answers

For "unhinged" mode, we use a completely different system prompt that instructs 
Grok to use ALL CAPS, meme language, fake statistics, sound effects, and 
break the fourth wall.

STEP 4: RESPONSE PARSING
Grok returns a script in this format:

ALEX: Hey Sam, what's the deal with this topic?
SAM: Well Alex, let me explain...
ALEX: That's fascinating! But why...
SAM: Great question. The reason is...

We parse this script to identify speakers for the TTS stage.


================================================================================
                       HOW GROK TTS API IS USED
================================================================================

After we have the script, we need to convert it to audio. Here's the process:

STEP 1: SCRIPT PARSING
We parse the script line by line, identifying each speaker:

parse_script("ALEX: Hello\nSAM: Hi") 
→ [('ALEX', 'Hello'), ('SAM', 'Hi')]

STEP 2: VOICE ASSIGNMENT
Each speaker gets a distinct Grok voice:
- ALEX → "Ara" (female voice)
- SAM → "Rex" (male voice)

STEP 3: SEQUENTIAL TTS GENERATION
For each line of dialogue, we make a separate API call:

POST https://api.x.ai/v1/audio/speech
{
  "input": "Hello, how are you today?",
  "voice": "Ara",
  "response_format": "mp3"
}

The API returns raw MP3 audio bytes. We concatenate all audio segments 
in order to create the final podcast.

STEP 4: BASE64 ENCODING
The concatenated audio is converted to a base64 data URL:
"data:audio/mpeg;base64,<base64_encoded_audio>"

This allows the browser to play the audio directly without needing 
a file server.

WHY WE DO IT THIS WAY:
- Each speaker has a distinct voice, making the podcast feel like a real 
  conversation between two people
- Sequential generation ensures proper dialogue flow
- MP3 format is universally supported in browsers
- Base64 encoding eliminates the need for file hosting


================================================================================
                    HOW GROK LIVE (REALTIME) API IS USED
================================================================================

Live Talk enables real-time voice conversation with Grok. This is more complex 
because it involves bidirectional audio streaming over WebSockets.

ARCHITECTURE:
Browser → Backend WebSocket Proxy → xAI Realtime API

WHY A PROXY?
Browser WebSockets cannot set custom HTTP headers. xAI requires Bearer token 
authentication in headers. So we proxy through our backend which can set headers.

STEP 1: EPHEMERAL TOKEN GENERATION
When Live Talk starts, we first get a short-lived token:

POST https://api.x.ai/v1/realtime/client_secrets
{
  "expires_after": {"seconds": 300}
}

Returns: {"value": "xai-realtime-client-secret-xxx", "expires_at": 1234567890}

STEP 2: WEBSOCKET CONNECTION
Our backend connects to xAI using this token:

wss://api.x.ai/v1/realtime
Headers: Authorization: Bearer <ephemeral_token>

STEP 3: SESSION CONFIGURATION
Once connected, we configure the voice session:

{
  "type": "session.update",
  "session": {
    "instructions": "You are Grok. The user is browsing: <page_title>. Be helpful.",
    "voice": "sage",
    "input_audio_format": "pcm16",
    "output_audio_format": "pcm16",
    "turn_detection": {"type": "server_vad"}
  }
}

STEP 4: AUDIO CAPTURE (Browser)
We use the Web Audio API to capture microphone input:
- getUserMedia() captures the mic stream
- AudioContext resamples to 24kHz mono
- ScriptProcessorNode converts Float32 samples to Int16 PCM
- PCM data is base64 encoded and sent over WebSocket

STEP 5: AUDIO TRANSMISSION
Microphone audio is sent as:
{
  "type": "input_audio_buffer.append",
  "audio": "<base64_pcm16_audio>"
}

STEP 6: SERVER-SIDE VAD
xAI's server detects when you start and stop speaking:
- "input_audio_buffer.speech_started" - you started talking
- "input_audio_buffer.speech_stopped" - you stopped talking

STEP 7: GROK'S RESPONSE
Grok processes your speech and responds with audio:
{
  "type": "response.output_audio.delta",
  "delta": "<base64_pcm16_audio_chunk>"
}

STEP 8: AUDIO PLAYBACK (Browser)
We decode and play Grok's audio in real-time:
- Base64 decode to bytes
- Convert Int16 PCM to Float32 samples
- Create AudioBuffer and play through AudioContext
- Schedule chunks sequentially for smooth playback


================================================================================
                      HOW CONTEXT BUILDING WORKS
================================================================================

The context builder (context_builder.py) is the brain that constructs prompts 
for Grok. It takes multiple inputs and creates a coherent prompt.

INPUTS:
1. page_text - Raw text from the webpage
2. page_title - The page's title tag
3. user_signals - X/Twitter data (interests, bio)
4. tone - User-selected tone (formal to unhinged)
5. target_duration - How long the podcast should be
6. mode - podcast, summary, or debate
7. x_influence - How much to incorporate X interests

DURATION CONFIGURATION:
We map duration to word counts based on ~150 words/minute speech rate:

Duration    Target Words    Lines (alternating speakers)
30sec       60              4
45sec       90              6
1min        140             8
2min        280             14
3min        420             20
5min        700             32
7min        1000            44
10min       1400            60

PROMPT CONSTRUCTION:
The prompt is built in layers:

1. MODE INSTRUCTION:
   - "podcast" → "Natural podcast conversation - Alex is curious, Sam is expert"
   - "summary" → "Quick summary - Alex asks, Sam explains key points"
   - "debate" → "Debate style - Alex and Sam take opposing views"

2. CONTENT SECTION:
   Topic: <page_title>
   CONTENT:
   <first 3000 chars of page_text>

3. X INTERESTS (if enabled):
   User interests to weave in naturally: AI, startups, technology
   User context: Tech enthusiast based in SF

4. REQUIREMENTS:
   - Write exactly 8 lines (alternating ALEX: and SAM:)
   - Keep each line SHORT (15-25 words max)
   - Target ~140 words total for 1min duration
   - Make it sound natural when spoken aloud

5. FORMAT INSTRUCTION:
   Format:
   ALEX: [sentence]
   SAM: [sentence]
   ...
   Start with ALEX:


================================================================================
                   HOW X (TWITTER) INTERESTS ARE USED
================================================================================

When the user enables "X Personalize", we fetch their Twitter/X data to 
customize the podcast content.

DATA FETCHED:
1. User bio/description
2. Recent liked tweets (up to 10)

API CALLS:
GET https://api.twitter.com/2/users/me
  → Returns user profile including bio

GET https://api.twitter.com/2/users/{id}/liked_tweets
  → Returns recent likes to infer interests

HOW INTERESTS ARE INCORPORATED:
The interests are injected into the prompt as additional context. For example, 
if the user likes AI and startup tweets, the prompt might include:

"User interests to weave in naturally: AI, machine learning, startups"

This causes Grok to naturally reference these topics when relevant. If the 
user is reading an article about apartments, the podcast might mention:

"ALEX: This location would be perfect for startup founders!"
"SAM: Exactly, it's close to the tech hub downtown."

The x_influence parameter controls how strongly these interests affect output:
- "minimal" → Barely mentioned
- "medium" → Occasionally referenced
- "strong" → Heavily incorporated


================================================================================
                   HOW BROWSER DATA IS EXTRACTED
================================================================================

When the user loads a webpage with Grokcaster installed, we can extract content 
in two ways:

METHOD 1: FULL PAGE EXTRACTION
When "Generate Podcast" is clicked, we grab:

document.body.innerText.slice(0, 10000)

This gets all visible text on the page, limited to 10,000 characters.

METHOD 2: SNIPPING TOOL (USER SELECTION)
The snipping tool lets users select specific content:

1. User clicks "Select Content" button
2. An overlay appears over the page
3. User clicks and drags to create a selection rectangle
4. On mouseup, we find all text elements within that rectangle:

   document.querySelectorAll('p, h1, h2, h3, li, span, div').forEach(el => {
     const rect = el.getBoundingClientRect();
     if (elementOverlapsSelection(rect, selectionRect)) {
       collectedText += el.innerText;
     }
   });

5. The selected text (up to 5000 chars) is stored
6. When generating, this snipped content is used instead of full page

WHY THIS MATTERS:
- Users can focus on specific sections (e.g., just the reviews, not the ads)
- More relevant content = better podcast
- Reduces noise from navigation, footers, sidebars


================================================================================
                      COMPLETE DATA FLOW
================================================================================

Here's the entire flow from user action to podcast playback:

1. USER OPENS EXTENSION
   → Content script injects UI into page via Shadow DOM

2. USER CONFIGURES OPTIONS
   → Selects mode (podcast/summary/debate)
   → Sets duration (30sec to 10min)
   → Sets tone (formal to unhinged)
   → Optionally enables X personalization
   → Optionally uses snipping tool

3. USER CLICKS "GENERATE PODCAST"
   → Frontend sends POST to /generate endpoint

4. BACKEND PROCESSES REQUEST
   a. If X enabled, fetch user interests from Twitter API
   b. Call context_builder to construct prompt
   c. Call Grok Chat API (grok-4-1-fast-non-reasoning) → Get script
   d. Parse script into speaker segments
   e. For each segment, call Grok TTS API → Get audio
   f. Concatenate audio, encode as base64
   g. Return {script, audio_url}

5. FRONTEND RECEIVES RESPONSE
   → Sets audio element src to data URL
   → Auto-plays audio
   → Shows animated border while playing

6. (OPTIONAL) USER STARTS LIVE TALK
   a. Frontend connects to backend WebSocket /live-ws
   b. Backend gets ephemeral token from xAI
   c. Backend connects to wss://api.x.ai/v1/realtime
   d. Frontend captures mic audio, sends to backend
   e. Backend forwards to xAI, forwards responses back
   f. Frontend plays Grok's voice responses in real-time


================================================================================
                    WHEN EACH GROK API IS USED
================================================================================

GROK CHAT API (grok-4-1-fast-non-reasoning)
├── Used: When generating podcast script
├── Trigger: User clicks "Generate Podcast"
├── Input: Constructed prompt with page content + settings
└── Output: Dialogue script (ALEX: ... SAM: ...)

GROK TTS API (Ara, Rex voices)
├── Used: Immediately after script generation
├── Trigger: Automatic, part of /generate flow
├── Input: Each line of dialogue + voice selection
└── Output: MP3 audio bytes

GROK REALTIME API (WebSocket)
├── Used: When user clicks "Live Talk with Grok"
├── Trigger: User initiates live conversation
├── Input: Real-time PCM16 audio from microphone
└── Output: Real-time PCM16 audio from Grok


================================================================================
                    TECHNICAL DIFFERENTIATORS
================================================================================

What makes Grokcaster technically impressive:

1. PURE GROK STACK
   No OpenAI, no ElevenLabs, no third-party AI. Everything runs on xAI's 
   Grok ecosystem: Chat for content, TTS for voice, Realtime for live.

2. TWO-VOICE PODCAST
   Unlike simple TTS, we parse scripts by speaker and use different Grok 
   voices (Ara/Rex), creating a genuine two-person podcast feel.

3. REAL-TIME BIDIRECTIONAL AUDIO
   Live Talk handles continuous audio streaming in both directions with 
   proper buffering, resampling, and playback scheduling.

4. CONTEXT-AWARE PERSONALIZATION
   We don't just summarize pages - we incorporate user interests from X, 
   creating truly personalized content.

5. BROWSER-NATIVE IMPLEMENTATION
   Everything runs in a Chrome extension with Shadow DOM isolation, 
   Web Audio API for audio processing, and WebSocket for real-time comms.

6. DYNAMIC DURATION CONTROL
   Users control exact podcast length, and we enforce word/line limits 
   in the prompt to achieve accurate durations.


================================================================================
                    FOR HACKATHON JUDGES
================================================================================

KEY TALKING POINTS:

"Grokcaster is a 100% Grok-native application. We use three xAI APIs working 
together: Grok Chat generates the script content, Grok TTS converts it to 
audio with two distinct voices, and Grok Realtime enables live voice 
conversations. No other AI services are involved."

"Our context builder is sophisticated - it takes webpage content, user 
preferences, X/Twitter interests, and desired tone to construct prompts 
that generate truly personalized podcast content."

"The Live Talk feature demonstrates real-time audio processing - we capture 
microphone input at 24kHz, convert to PCM16, stream over WebSocket to Grok's 
Realtime API, and play back responses with proper audio scheduling for 
seamless conversation."

"We solved the browser WebSocket authentication problem by building a proxy 
that handles ephemeral token generation and secure header injection, allowing 
browser-based real-time voice without exposing API keys."


================================================================================
                         SUMMARY
================================================================================

Grokcaster demonstrates the full potential of xAI's Grok ecosystem:

┌─────────────────────────────────────────────────────────────────────┐
│                        GROK ECOSYSTEM USAGE                         │
├─────────────────────────────────────────────────────────────────────┤
│  Content Generation  →  Grok Chat API (grok-4-1-fast-non-reasoning) │
│  Voice Synthesis     →  Grok TTS API (Ara + Rex voices)            │
│  Live Conversation   →  Grok Realtime API (WebSocket + PCM16)      │
│  Personalization     →  X/Twitter API (optional)                   │
└─────────────────────────────────────────────────────────────────────┘

Every AI-powered feature in Grokcaster is powered by Grok. We've built a 
complete podcast generation and live conversation platform using only 
xAI's native APIs, demonstrating what's possible with the Grok ecosystem.

================================================================================
                    END OF TECHNICAL DEEP DIVE
================================================================================

